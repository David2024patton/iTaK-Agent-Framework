<p align="center">
  <img src="https://img.shields.io/badge/iTaK-Agent_Framework-blue?style=for-the-badge&logo=robot" alt="iTaK Agent Framework">
</p>

<h1 align="center">iTaK Agent Framework</h1>

<p align="center">
  <strong>Intelligent Task Automation Kernel</strong><br>
  A powerful multi-agent automation framework with self-healing capabilities and 10-layer architecture.
</p>

<p align="center">
  <a href="#-features">Features</a> ‚Ä¢
  <a href="#-quick-start">Quick Start</a> ‚Ä¢
  <a href="#-remote-access">Remote Access</a> ‚Ä¢
  <a href="#-architecture">Architecture</a> ‚Ä¢
  <a href="#-model-catalog">Models</a>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10%2B-blue?logo=python" alt="Python 3.10+">
  <img src="https://img.shields.io/badge/License-MIT-green" alt="MIT License">
  <img src="https://img.shields.io/badge/Ollama-Supported-orange?logo=ollama" alt="Ollama Ready">
</p>

---

## üöÄ What is iTaK?

iTaK (Intelligent Task Automation Kernel) is a **production-ready multi-agent framework** that turns any AI into an autonomous coding assistant with:

- **üîß Self-Healing** - Automatically detects and recovers from failures
- **üß† Persistent Memory** - Remembers past solutions via ChromaDB
- **üê≥ Safe Execution** - Runs untrusted code in Docker sandboxes
- **üåê Remote Access** - Access your local AI from anywhere via VPS or Cloudflare tunnels
- **üè† Local-First** - Optimized for Ollama and local LLMs

---

## ‚ú® Features

| Feature | Description |
|---------|-------------|
| **One-Command Setup** | `npm install` auto-configures Docker, Ollama, and all services |
| **10-Layer Architecture** | Specialized agents for each phase of development |
| **Remote Access** | Access local AI from anywhere via [VPS tunnels](docs/VPS_SETUP.md) or Cloudflare |
| **API Gateway** | FastAPI gateway with Playwright, SearXNG, and ChromaDB |
| **Circuit Breaker** | Prevents cascade failures with automatic recovery |
| **Docker Sandbox** | Execute code safely in isolated containers |
| **LLM Tracer** | Log all interactions for fine-tuning |

---

## üì¶ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/David2024patton/iTaK-Agent-Framework.git
cd iTaK-Agent-Framework

# One command does everything
npm install
```

**`npm install` automatically:**
- ‚úÖ Detects your OS (Windows/Mac/Linux)
- ‚úÖ Installs Docker Desktop and Ollama (if needed)
- ‚úÖ Starts all containers (Ollama, ChromaDB, Playwright, SearXNG)
- ‚úÖ Pulls the default LLM model (`qwen3-vl:2b`)
- ‚úÖ Generates `.env` with service URLs
- ‚úÖ Launches `itak` CLI

### Using iTaK

After install, just run:

```bash
itak
```

**Main Menu:**
```
[1] üåê Web App      - HTML/CSS/JS web application
[2] üêç Python       - Python script or automation
[3] ‚ö° API/Backend  - REST API or backend service
[4] ü§ñ AI Agent     - AI agent or workflow
[5] üìù Custom       - Describe your project freely
[6] üí¨ Chat         - Interactive coding assistance

[7] ‚ö° API Gateway  - Manage tunnels and remote access
```

---

## üåê Remote Access

Access your local AI services from anywhere - even without a static IP.

### Option 1: Cloudflare Tunnel (Easiest)

No VPS or account needed! From iTaK:

```bash
itak
> /api
# Select [2] Cloudflare Tunnel (Quick)
```

Gets you a public URL like `https://random-name.trycloudflare.com`

### Option 2: VPS + FRP Tunnel (Best for Production)

For permanent remote access with your own domain:

**First-time setup:**

1. **Set up VPS** ‚Üí [docs/VPS_SETUP.md](docs/VPS_SETUP.md)
2. **Copy your auth token** from `.env` ‚Üí `FRP_AUTH_TOKEN=xxx`
3. **Configure VPS in iTaK:**
   ```bash
   itak
   > /api
   # Select [4] Configure VPS Connection
   # Enter your VPS IP and auth token
   ```
4. **Start tunnel:**
   ```bash
   # Select [5] Start/Stop FRP Tunnel
   ```

**Auto-reconnect:** Once configured, the FRP tunnel will **automatically start** on every `npm install` or Docker restart. No need to manually start it again!

**Your VPS endpoints:**

| Service | URL |
|---------|-----|
| Ollama LLM | `http://YOUR_VPS_IP:11434` |
| ChromaDB | `http://YOUR_VPS_IP:29800` |
| Playwright | `http://YOUR_VPS_IP:39281` |
| SearXNG | `http://YOUR_VPS_IP:48192` |

---

## üîå Use in Your IDE

Your local Ollama is **OpenAI-compatible**! Use it with any IDE that supports custom OpenAI endpoints.

### Settings for Any IDE

| Setting | Value |
|---------|-------|
| **Base URL** | `http://localhost:11434/v1` |
| **API Key** | `ollama` (any string works) |
| **Model** | `qwen3-vl:2b` |

### VS Code / Cursor / Continue.dev

```json
{
  "openai.baseUrl": "http://localhost:11434/v1",
  "openai.apiKey": "ollama",
  "openai.model": "qwen3-vl:2b"
}
```

### JetBrains AI Assistant

1. Settings ‚Üí AI Assistant ‚Üí Custom Provider
2. Enter Base URL: `http://localhost:11434/v1`
3. Model: `qwen3-vl:2b`

### Test with cURL

```bash
curl http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"qwen3-vl:2b","messages":[{"role":"user","content":"Hello!"}]}'
```

Works with: **VS Code, Cursor, JetBrains, Neovim, Emacs, Obsidian, and any OpenAI client!**

---

## üéÆ GPU Acceleration

If you have an NVIDIA GPU, Ollama will **automatically use CUDA** for faster inference. The setup script detects your GPU and confirms acceleration in the `.env` file.

---

## üèóÔ∏è Architecture

iTaK uses a **10-layer architecture** where each layer has a specific role:

| Layer | Name | Role |
|:-----:|------|------|
| 1 | **Analyst** | Parse intent, gather requirements |
| 2 | **Recon** | Research, web scraping, data gathering |
| 3 | **Orchestrator** | Plan and delegate work |
| 4 | **Builder** | Write code and content |
| 5 | **Validator** | Test, lint, verify quality |
| 6 | **Sandbox** | Execute code safely in Docker |
| 7 | **Deployer** | Deploy to production |
| 8 | **Librarian** | Manage memory and skills |
| 9 | **Healer** | Monitor, detect failures, auto-recover |
| 10 | **Swarm** | Multi-agent collaboration |

---

## üìÅ Project Structure

```
iTaK-Agent-Framework/
‚îú‚îÄ‚îÄ src/itak/              # Core framework
‚îÇ   ‚îú‚îÄ‚îÄ cli/               # Command line interface
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api_manager.py # API Gateway manager
‚îÇ   ‚îú‚îÄ‚îÄ layers/            # 10-layer architecture
‚îÇ   ‚îú‚îÄ‚îÄ utilities/         # Circuit breaker, helpers
‚îÇ   ‚îî‚îÄ‚îÄ security/          # Docker sandbox
‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îî‚îÄ‚îÄ api-gateway/       # Docker compose for all services
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ VPS_SETUP.md       # Remote access setup guide
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ postinstall.js     # Auto-setup script
‚îî‚îÄ‚îÄ README.md
```

---

## üê≥ Docker Services

After `npm install`, these containers run under the `api-gateway` project:

| Service | Port | Description |
|---------|------|-------------|
| **Ollama** | 11434 | Local LLM server |
| **ChromaDB** | 29800 | Vector memory database |
| **Redis** | 63790 | Caching, queues, sessions |
| **Whisper** | 69247 | Speech-to-text (GPU) |
| **Playwright** | 39281 | Browser automation |
| **SearXNG** | 48192 | Meta-search (finds URLs) |
| **Crawl4AI** | 47836 | Web scraper (content for LLMs) |
| **ComfyUI** | 58127 | AI image generation (GPU) |
| **Supabase DB** | 54321 | PostgreSQL database |
| **Supabase Studio** | 54323 | Database admin UI |

> **SearXNG vs Crawl4AI:** SearXNG finds URLs from search queries. Crawl4AI extracts full page content as markdown for LLMs.

---

## ü§ñ Model Catalog

iTaK includes a curated catalog of **100+ models** across 20 domain categories. Run `itak` and select a project type to get matched with the best model for your task.

**Popular Models:**
- `qwen3-vl:2b` - Default, fast vision + text
- `qwen2.5-coder:7b` - Specialized for coding
- `deepseek-r1:8b` - Deep reasoning

---

## ü§ù Acknowledgments

Built upon [CrewAI](https://github.com/crewAIInc/crewAI) (MIT License).

---

## üìÑ License

MIT License - see [LICENSE](licenses/LICENSE)
